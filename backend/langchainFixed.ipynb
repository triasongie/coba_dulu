{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53c993c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Using cached langchain-1.1.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: langchain_core in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (1.1.0)\n",
      "Collecting langgraph<1.1.0,>=1.0.2 (from langchain)\n",
      "  Downloading langgraph-1.0.4-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from langchain) (2.12.5)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from langchain_core) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from langchain_core) (0.4.49)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from langchain_core) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from langchain_core) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from langchain_core) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from langchain_core) (4.15.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain_core) (3.0.0)\n",
      "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
      "  Using cached langgraph_checkpoint-3.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
      "  Using cached langgraph_prebuilt-1.0.5-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
      "  Using cached langgraph_sdk-0.2.10-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting xxhash>=3.5.0 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
      "  Downloading xxhash-3.6.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: anyio in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (4.11.0)\n",
      "Requirement already satisfied: certifi in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (1.0.9)\n",
      "Requirement already satisfied: idna in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (0.16.0)\n",
      "Collecting ormsgpack>=1.12.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain)\n",
      "  Downloading ormsgpack-1.12.0-cp312-cp312-win_amd64.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain_core) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain_core) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (1.3.1)\n",
      "Using cached langchain-1.1.0-py3-none-any.whl (101 kB)\n",
      "Downloading langgraph-1.0.4-py3-none-any.whl (157 kB)\n",
      "Using cached langgraph_checkpoint-3.0.1-py3-none-any.whl (46 kB)\n",
      "Using cached langgraph_prebuilt-1.0.5-py3-none-any.whl (35 kB)\n",
      "Using cached langgraph_sdk-0.2.10-py3-none-any.whl (58 kB)\n",
      "Downloading xxhash-3.6.0-cp312-cp312-win_amd64.whl (31 kB)\n",
      "Downloading ormsgpack-1.12.0-cp312-cp312-win_amd64.whl (112 kB)\n",
      "Installing collected packages: xxhash, ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph, langchain\n",
      "Successfully installed langchain-1.1.0 langgraph-1.0.4 langgraph-checkpoint-3.0.1 langgraph-prebuilt-1.0.5 langgraph-sdk-0.2.10 ormsgpack-1.12.0 xxhash-3.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_classic\n",
      "  Downloading langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from langchain_classic) (1.1.0)\n",
      "Collecting langchain-text-splitters<2.0.0,>=1.0.0 (from langchain_classic)\n",
      "  Downloading langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.1.17 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from langchain_classic) (0.4.49)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from langchain_classic) (2.12.5)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from langchain_classic) (6.0.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from langchain_classic) (2.32.5)\n",
      "Collecting sqlalchemy<3.0.0,>=1.4.0 (from langchain_classic)\n",
      "  Downloading sqlalchemy-2.0.44-cp312-cp312-win_amd64.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain_classic) (1.33)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain_classic) (25.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain_classic) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain_classic) (4.15.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from langsmith<1.0.0,>=0.1.17->langchain_classic) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from langsmith<1.0.0,>=0.1.17->langchain_classic) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from langsmith<1.0.0,>=0.1.17->langchain_classic) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from langsmith<1.0.0,>=0.1.17->langchain_classic) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain_classic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain_classic) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain_classic) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->langchain_classic) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->langchain_classic) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->langchain_classic) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->langchain_classic) (2025.11.12)\n",
      "Collecting greenlet>=1 (from sqlalchemy<3.0.0,>=1.4.0->langchain_classic)\n",
      "  Downloading greenlet-3.2.4-cp312-cp312-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: anyio in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.17->langchain_classic) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.17->langchain_classic) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.17->langchain_classic) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain_classic) (3.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.17->langchain_classic) (1.3.1)\n",
      "Downloading langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.3/1.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.3/1.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.3/1.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.3/1.0 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 0.5/1.0 MB 322.8 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 0.5/1.0 MB 322.8 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 0.8/1.0 MB 385.8 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 0.8/1.0 MB 385.8 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 0.8/1.0 MB 385.8 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 0.8/1.0 MB 385.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.0/1.0 MB 366.4 kB/s eta 0:00:00\n",
      "Downloading langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\n",
      "Downloading sqlalchemy-2.0.44-cp312-cp312-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.1 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.1 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.1 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.5/2.1 MB 390.1 kB/s eta 0:00:05\n",
      "   --------- ------------------------------ 0.5/2.1 MB 390.1 kB/s eta 0:00:05\n",
      "   --------- ------------------------------ 0.5/2.1 MB 390.1 kB/s eta 0:00:05\n",
      "   -------------- ------------------------- 0.8/2.1 MB 390.1 kB/s eta 0:00:04\n",
      "   -------------- ------------------------- 0.8/2.1 MB 390.1 kB/s eta 0:00:04\n",
      "   -------------- ------------------------- 0.8/2.1 MB 390.1 kB/s eta 0:00:04\n",
      "   ------------------- -------------------- 1.0/2.1 MB 399.4 kB/s eta 0:00:03\n",
      "   ------------------- -------------------- 1.0/2.1 MB 399.4 kB/s eta 0:00:03\n",
      "   ------------------- -------------------- 1.0/2.1 MB 399.4 kB/s eta 0:00:03\n",
      "   ------------------------ --------------- 1.3/2.1 MB 399.5 kB/s eta 0:00:03\n",
      "   ------------------------ --------------- 1.3/2.1 MB 399.5 kB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 1.6/2.1 MB 427.9 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.6/2.1 MB 427.9 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.6/2.1 MB 427.9 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.6/2.1 MB 427.9 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 405.9 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 405.9 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 405.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  2.1/2.1 MB 394.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 389.6 kB/s eta 0:00:00\n",
      "Downloading greenlet-3.2.4-cp312-cp312-win_amd64.whl (299 kB)\n",
      "Installing collected packages: greenlet, sqlalchemy, langchain-text-splitters, langchain_classic\n",
      "Successfully installed greenlet-3.2.4 langchain-text-splitters-1.0.0 langchain_classic-1.0.0 sqlalchemy-2.0.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# RUN SCRIPT TRAIN DULU SEBELUM RUN YANG INI\n",
    "\n",
    "!pip install -q langchain-google-genai\n",
    "!pip install -U --upgrade langchain langchain_core \n",
    "!pip install langchain_classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ae86e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rdkit in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (2025.9.1)\n",
      "Requirement already satisfied: numpy in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from rdkit) (2.3.5)\n",
      "Requirement already satisfied: Pillow in d:\\asah 2025\\capstone\\langchain\\llmenv\\lib\\site-packages (from rdkit) (12.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bf5b154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any, Optional\n",
    "import json\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain.tools import tool\n",
    "from langchain_classic.agents import initialize_agent, AgentType\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw, AllChem\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56abbc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jangan lupa set api key dulu\n",
    "\n",
    "API_KEY = os.getenv(\"GOOGLE_API_KEY\", \"AIzaSyCGCf8Tx_kKuncajAg_pOffgntPeMhD0FQ\")\n",
    "llm = GoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\", \n",
    "    temperature=0.7,\n",
    "    google_api_key=API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73530e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded model from multitarget_model.pkl\n"
     ]
    }
   ],
   "source": [
    "class PropsRegressor:\n",
    "    def __init__(self, model_path: Optional[str] = None):\n",
    "        self.model = None\n",
    "        self.property_names = [\"pic50\", \"logp\", \"atoms\"]\n",
    "        \n",
    "        if model_path and os.path.exists(model_path):\n",
    "            import joblib\n",
    "            self.model = joblib.load(model_path)\n",
    "            print(f\"‚úì Loaded model from {model_path}\")\n",
    "    \n",
    "    def smiles_to_fingerprint(self, smiles: str, radius: int = 2, n_bits: int = 2048) -> Optional[np.ndarray]:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return None\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)\n",
    "        return np.array(fp)\n",
    "    \n",
    "    def predict(self, smiles: str) -> Optional[Dict[str, float]]:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return None\n",
    "        \n",
    "        if self.model is None:\n",
    "            return {\n",
    "                \"pic50\": None,\n",
    "                \"logp\": None,\n",
    "                \"atoms\": mol.GetNumAtoms()\n",
    "            }\n",
    "        \n",
    "        fp = self.smiles_to_fingerprint(smiles)\n",
    "        if fp is None:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            predictions = self.model.predict([fp])[0]\n",
    "            return {\n",
    "                \"pic50\": float(predictions[0]),\n",
    "                \"logp\": float(predictions[1]),\n",
    "                \"atoms\": int(round(predictions[2]))\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Prediction error: {e}\")\n",
    "            return None\n",
    "\n",
    "predictor = PropsRegressor(model_path=\"multitarget_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e330632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_constraints(text: str) -> Dict[str, tuple]:\n",
    "    \"\"\"Parse constraint text into min/max ranges.\"\"\"\n",
    "    constraints = {}\n",
    "    text = text.lower().replace(\":\", \" \").replace(\",\", \"\\n\")\n",
    "    \n",
    "    for line in text.strip().split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        parts = line.split()\n",
    "        if len(parts) < 2:\n",
    "            continue\n",
    "        \n",
    "        prop_name = parts[0]\n",
    "        for part in parts[1:]:\n",
    "            if \"-\" in part and not part.startswith(\"-\"):\n",
    "                try:\n",
    "                    min_val, max_val = part.split(\"-\")\n",
    "                    constraints[prop_name] = (float(min_val), float(max_val))\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    return constraints\n",
    "\n",
    "\n",
    "def validate_properties(props: Dict[str, float], constraints: Dict[str, tuple]) -> bool:\n",
    "    \"\"\"Check if all properties satisfy constraints.\"\"\"\n",
    "    for prop_name, (min_val, max_val) in constraints.items():\n",
    "        if prop_name not in props or props[prop_name] is None:\n",
    "            return False\n",
    "        if not (min_val <= props[prop_name] <= max_val):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7b3c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateInput(BaseModel):\n",
    "    constraints: str\n",
    "\n",
    "@tool(args_schema=GenerateInput)\n",
    "def generate_and_validate_molecules(constraints: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate SMILES and validate against constraints.\n",
    "    Returns JSON with molecule data (without justifications/images yet).\n",
    "    \"\"\"\n",
    "    constraint_dict = parse_constraints(constraints)\n",
    "    if not constraint_dict:\n",
    "        return json.dumps({\"error\": \"Could not parse constraints\"}, indent=2)\n",
    "    \n",
    "    gen_prompt = f\"\"\"Generate 5 potentially novel chemically valid SMILES strings for drug-like molecules.\n",
    "\n",
    "        Target properties:\n",
    "        {constraints}\n",
    "\n",
    "        Return ONLY SMILES strings, one per line. NO explanations, NO numbering.\n",
    "\n",
    "        Example format:\n",
    "        CCO\n",
    "        c1ccccc1\n",
    "        CC(C)O\n",
    "        \"\"\"\n",
    "\n",
    "    try:\n",
    "        smiles_raw = llm.invoke(gen_prompt)\n",
    "        smiles_list = [\n",
    "            s.strip() \n",
    "            for s in smiles_raw.split(\"\\n\") \n",
    "            if s.strip() and not any(s.strip().startswith(x) for x in [\"#\", \"```\", \"1.\", \"2.\", \"3.\"])\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"LLM generation failed: {str(e)}\"}, indent=2)\n",
    "    \n",
    "    results = []\n",
    "    valid_count = 0\n",
    "    \n",
    "    for i, smiles in enumerate(smiles_list[:5], 1):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            results.append({\n",
    "                \"id\": i,\n",
    "                \"smiles\": smiles,\n",
    "                \"valid\": False,\n",
    "                \"error\": \"Invalid SMILES structure\"\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        props = predictor.predict(smiles)\n",
    "        if props is None or any(v is None for v in props.values()):\n",
    "            results.append({\n",
    "                \"id\": i,\n",
    "                \"smiles\": smiles,\n",
    "                \"valid\": False,\n",
    "                \"error\": \"Property prediction failed\"\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        is_valid = validate_properties(props, constraint_dict)\n",
    "        if is_valid:\n",
    "            valid_count += 1\n",
    "        \n",
    "        results.append({\n",
    "            \"id\": i,\n",
    "            \"smiles\": smiles,\n",
    "            \"valid\": is_valid,\n",
    "            \"properties\": {\n",
    "                \"pIC50\": round(props[\"pic50\"], 2),\n",
    "                \"logP\": round(props[\"logp\"], 2),\n",
    "                \"atoms\": props[\"atoms\"]\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    return json.dumps({\n",
    "        \"constraints\": constraint_dict,\n",
    "        \"total_generated\": len(results),\n",
    "        \"valid_molecules\": valid_count,\n",
    "        \"results\": results\n",
    "    }, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f7ba522",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JustifyInput(BaseModel):\n",
    "    smiles: str\n",
    "    properties: dict\n",
    "    constraints: dict\n",
    "\n",
    "@tool(args_schema=JustifyInput)\n",
    "def generate_justification(smiles: str, properties: dict, constraints: dict) -> str:\n",
    "    \"\"\"\n",
    "    Generate scientific justification for why molecule satisfies constraints.\n",
    "    Uses LLM to create natural language explanation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build context for LLM\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return \"Error: Invalid SMILES structure\"\n",
    "    \n",
    "    from rdkit.Chem import Descriptors\n",
    "    formula = Chem.rdMolDescriptors.CalcMolFormula(mol)\n",
    "    mw = Descriptors.MolWt(mol)\n",
    "    \n",
    "    prompt = f\"\"\"You are a medicinal chemist. Provide a brief scientific justification for why this molecule is a valid candidate.\n",
    "\n",
    "SMILES: {smiles}\n",
    "Molecular Formula: {formula}\n",
    "Molecular Weight: {mw:.2f} g/mol\n",
    "\n",
    "Predicted Properties:\n",
    "- pIC50: {properties.get('pIC50', 'N/A')}\n",
    "- logP: {properties.get('logP', 'N/A')}\n",
    "- Atom count: {properties.get('atoms', 'N/A')}\n",
    "\n",
    "Target Constraints:\n",
    "{json.dumps(constraints, indent=2)}\n",
    "\n",
    "Provide a 2-3 sentence justification explaining:\n",
    "1. Why the predicted properties satisfy the constraints\n",
    "2. What structural features contribute to these properties\n",
    "3. Brief assessment of drug-likeness\n",
    "\n",
    "Keep it concise and scientific. Do NOT include the SMILES or property values again.\"\"\"\n",
    "\n",
    "    try:\n",
    "        justification = llm.invoke(prompt)\n",
    "        return justification.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error generating justification: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc921380",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageInput(BaseModel):\n",
    "    smiles: str\n",
    "\n",
    "@tool(args_schema=ImageInput)\n",
    "def generate_molecule_image(smiles: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate 2D molecular structure image from SMILES.\n",
    "    Returns base64-encoded PNG.\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return \"Error: Invalid SMILES structure\"\n",
    "    \n",
    "    try:\n",
    "        img = Draw.MolToImage(mol, size=(400, 400))\n",
    "        buf = BytesIO()\n",
    "        img.save(buf, format=\"PNG\")\n",
    "        b64_data = base64.b64encode(buf.getvalue()).decode()\n",
    "        \n",
    "        return json.dumps({\n",
    "            \"success\": True,\n",
    "            \"image_base64\": b64_data,\n",
    "            \"format\": \"PNG\",\n",
    "            \"size\": \"400x400\",\n",
    "            \"data_length\": len(b64_data)\n",
    "        })\n",
    "    except Exception as e:\n",
    "        return json.dumps({\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaeebd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_18312\\1511604827.py:9: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the [LangGraph documentation](https://langchain-ai.github.io/langgraph/) as well as guides for [Migrating from AgentExecutor](https://python.langchain.com/docs/how_to/migrate_agent/) and LangGraph's [Pre-built ReAct agent](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/).\n",
      "  agent = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "# Set  up Agent\n",
    "\n",
    "tools = [\n",
    "    generate_and_validate_molecules,\n",
    "    generate_justification,\n",
    "    generate_molecule_image,\n",
    "]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    max_iterations=10,  # Higher limit since we need multiple tool calls\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aa707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ini buat test display-nya doang, kayanya kalo ke web ga butuh\n",
    "\n",
    "def display_results(results: Dict[str, Any]):\n",
    "    \"\"\"Pretty print results with justifications and images.\"\"\"\n",
    "    if \"error\" in results:\n",
    "        print(f\"\\n‚ùå Error: {results['error']}\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nConstraints: {results.get('constraints', {})}\")\n",
    "    print(f\"Total Generated: {results.get('total_generated', 0)}\")\n",
    "    print(f\"Valid Molecules: {results.get('valid_molecules', 0)}\")\n",
    "    \n",
    "    valid_count = 0\n",
    "    for molecule in results.get(\"results\", []):\n",
    "        if not molecule.get(\"valid\", False):\n",
    "            continue\n",
    "        \n",
    "        valid_count += 1\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"VALID MOLECULE #{valid_count}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(f\"\\nüìù SMILES: {molecule['smiles']}\")\n",
    "        \n",
    "        print(f\"\\nüìä Predicted Properties:\")\n",
    "        for prop, value in molecule.get(\"properties\", {}).items():\n",
    "            print(f\"   {prop}: {value}\")\n",
    "        \n",
    "        if \"justification\" in molecule:\n",
    "            print(f\"\\nüí° Scientific Justification:\")\n",
    "            print(f\"   {molecule['justification']}\")\n",
    "        \n",
    "        if \"image_base64\" in molecule:\n",
    "            img_len = len(molecule[\"image_base64\"])\n",
    "            print(f\"\\nüñºÔ∏è  2D Structure Image:\")\n",
    "            print(f\"   Format: PNG\")\n",
    "            print(f\"   Base64 length: {img_len} characters\")\n",
    "            print(f\"   (Decode to view or save as .png file)\")\n",
    "        \n",
    "        if \"image_error\" in molecule:\n",
    "            print(f\"\\n‚ö†Ô∏è  Image Error: {molecule['image_error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e1374af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(results: Dict[str, Any], output_dir: str = \".\"):\n",
    "    \"\"\"Save results to files.\"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Save JSON\n",
    "    json_path = os.path.join(output_dir, \"results_with_justifications.json\")\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"\\n‚úì Saved JSON to: {json_path}\")\n",
    "    \n",
    "    # Save images\n",
    "    for molecule in results.get(\"results\", []):\n",
    "        if molecule.get(\"valid\") and \"image_base64\" in molecule:\n",
    "            img_data = base64.b64decode(molecule[\"image_base64\"])\n",
    "            img_path = os.path.join(output_dir, f\"molecule_{molecule['id']}.png\")\n",
    "            with open(img_path, \"wb\") as f:\n",
    "                f.write(img_data)\n",
    "            print(f\"‚úì Saved image to: {img_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f4593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_manual_orchestration(constraints: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Orchestrating agent manually since the tools are deterministic\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"Manual Orchestration with LLM Justifications\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Generating and validating SMILES\n",
    "    print(\"\\nStep 1: Generating and validating molecules...\")\n",
    "    result_json = generate_and_validate_molecules.invoke({\n",
    "        \"constraints\": constraints,\n",
    "    })\n",
    "    \n",
    "    try:\n",
    "        results = json.loads(result_json)\n",
    "    except:\n",
    "        return {\"error\": \"Failed to parse generation results\"}\n",
    "    \n",
    "    if \"error\" in results:\n",
    "        return results\n",
    "    \n",
    "    # Generate Justification and image\n",
    "    print(f\"\\nStep 2: Processing {results['valid_molecules']} valid molecules...\")\n",
    "    \n",
    "    for i, molecule in enumerate(results[\"results\"], 1):\n",
    "        if not molecule.get(\"valid\", False):\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n  Processing molecule {i}...\")\n",
    "        \n",
    "        # Generate justification (LLM call)\n",
    "        print(f\"    - Generating justification...\")\n",
    "        justification = generate_justification.invoke({\n",
    "            \"smiles\": molecule[\"smiles\"],\n",
    "            \"properties\": molecule[\"properties\"],\n",
    "            \"constraints\": results[\"constraints\"]\n",
    "        })\n",
    "        molecule[\"justification\"] = justification\n",
    "        \n",
    "        # Generate image (RDkit)\n",
    "        print(f\"    - Generating image...\")\n",
    "        image_result = generate_molecule_image.invoke({\n",
    "            \"smiles\": molecule[\"smiles\"]\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            image_data = json.loads(image_result)\n",
    "            if image_data.get(\"success\"):\n",
    "                molecule[\"image_base64\"] = image_data[\"image_base64\"]\n",
    "                molecule[\"image_info\"] = {\n",
    "                    \"format\": image_data.get(\"format\", \"PNG\"),\n",
    "                    \"size\": image_data.get(\"size\", \"400x400\")\n",
    "                }\n",
    "        except:\n",
    "            molecule[\"image_error\"] = \"Failed to generate image\"\n",
    "    \n",
    "    print(\"\\n‚úì Processing complete!\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5c38c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RECOMMENDED APPROACH: Manual Orchestration\n",
      "================================================================================\n",
      "================================================================================\n",
      "Manual Orchestration with LLM Justifications\n",
      "================================================================================\n",
      "\n",
      "Step 1: Generating and validating molecules...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:28:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:28:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:28:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:28:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:28:52] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:28:52] DEPRECATION WARNING: please use MorganGenerator\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2: Processing 2 valid molecules...\n",
      "\n",
      "  Processing molecule 4...\n",
      "    - Generating justification...\n",
      "    - Generating image...\n",
      "\n",
      "  Processing molecule 5...\n",
      "    - Generating justification...\n",
      "    - Generating image...\n",
      "\n",
      "‚úì Processing complete!\n",
      "\n",
      "================================================================================\n",
      "RESULTS\n",
      "================================================================================\n",
      "\n",
      "Constraints: {'pic50': [0.1, 0.5], 'logp': [1.0, 2.0], 'atoms': [10.0, 30.0]}\n",
      "Total Generated: 6\n",
      "Valid Molecules: 2\n",
      "\n",
      "================================================================================\n",
      "VALID MOLECULE #1\n",
      "================================================================================\n",
      "\n",
      "üìù SMILES: O=C(N1CCOCC1)c1ccccc1\n",
      "\n",
      "üìä Predicted Properties:\n",
      "   pIC50: 0.31\n",
      "   logP: 1.09\n",
      "   atoms: 15\n",
      "\n",
      "üí° Scientific Justification:\n",
      "   This molecule is a valid candidate as its predicted pIC50, logP, and atom count all fall within the specified target constraints, indicating a compound with suitable size, lipophilicity, and minimal target activity. The phenyl and morpholine rings, alongside the amide functionality, contribute to the molecule's overall size and lipophilicity, while the polar features are likely key to the observed activity and favorable drug-likeness profile.\n",
      "\n",
      "üñºÔ∏è  2D Structure Image:\n",
      "   Format: PNG\n",
      "   Base64 length: 10612 characters\n",
      "   (Decode to view or save as .png file)\n",
      "\n",
      "================================================================================\n",
      "VALID MOLECULE #2\n",
      "================================================================================\n",
      "\n",
      "üìù SMILES: CN(C)C(=O)Cc1ccccc1\n",
      "\n",
      "üìä Predicted Properties:\n",
      "   pIC50: 0.27\n",
      "   logP: 1.6\n",
      "   atoms: 14\n",
      "\n",
      "üí° Scientific Justification:\n",
      "   This molecule is a valid candidate as its predicted pIC50, logP, and atom count successfully meet all specified target constraints. The presence of a phenyl ring and N,N-dimethylamide moiety contributes to its balanced lipophilicity and appropriate atom count, ensuring favorable physicochemical properties. Overall, the molecule exhibits good drug-likeness due to its low molecular weight and optimal logP, suggesting potential for good ADME characteristics.\n",
      "\n",
      "üñºÔ∏è  2D Structure Image:\n",
      "   Format: PNG\n",
      "   Base64 length: 15528 characters\n",
      "   (Decode to view or save as .png file)\n",
      "\n",
      "‚úì Saved JSON to: .\\results_with_justifications.json\n",
      "‚úì Saved image to: .\\molecule_4.png\n",
      "‚úì Saved image to: .\\molecule_5.png\n"
     ]
    }
   ],
   "source": [
    "# Uji Coba\n",
    "# FEBE TLG BACA: kalo diliat, input yg diterima si agent tuh bentuk teks. Buat input sbnrnya aku \n",
    "# serahin ke kalian karna keknya kalian lebih ngerti, cm aku kepikiran input kita diubah ke bentuk\n",
    "# teks kaya yg constraint di bawah ini\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "# From this\n",
    "# constraint_text = f\"\"\"\n",
    "# pIC50 {pic50_min}-{pic50_max} \n",
    "# LogP {logp_min}-{logp_max}\n",
    "# atoms {atom_min}-{atom_max}\n",
    "\n",
    "# To this \n",
    "    constraints = \"\"\"\n",
    "    pIC50: 0.1-0.5\n",
    "    logP: 1.0-2.0\n",
    "    atoms: 10-30\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Running Chemical Discovery Agent\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    results = run_with_manual_orchestration(\n",
    "        constraints=constraints,\n",
    "    )\n",
    "    \n",
    "    display_results(results)\n",
    "    save_results(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
